{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlpclassifier on iris.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Il70WdqXxV2c"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.neural_network import MLPClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()\n",
        "x=iris.data\n",
        "y=iris.target"
      ],
      "metadata": {
        "id": "UdkulEJUySgc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= MLPClassifier(hidden_layer_sizes=(200),max_iter=200,activation='relu',verbose=True)\n"
      ],
      "metadata": {
        "id": "dJjwEIru5Qw4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=0)\n",
        "model.fit(x_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjQb0erG7XTb",
        "outputId": "7d4d995c-6674-4108-be43-5679f87bf67f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.16875836\n",
            "Iteration 2, loss = 1.13639710\n",
            "Iteration 3, loss = 1.10647326\n",
            "Iteration 4, loss = 1.07731682\n",
            "Iteration 5, loss = 1.04878069\n",
            "Iteration 6, loss = 1.02109427\n",
            "Iteration 7, loss = 0.99454644\n",
            "Iteration 8, loss = 0.96916379\n",
            "Iteration 9, loss = 0.94472188\n",
            "Iteration 10, loss = 0.92106911\n",
            "Iteration 11, loss = 0.89828721\n",
            "Iteration 12, loss = 0.87637107\n",
            "Iteration 13, loss = 0.85531146\n",
            "Iteration 14, loss = 0.83527412\n",
            "Iteration 15, loss = 0.81624588\n",
            "Iteration 16, loss = 0.79813967\n",
            "Iteration 17, loss = 0.78087309\n",
            "Iteration 18, loss = 0.76443373\n",
            "Iteration 19, loss = 0.74886288\n",
            "Iteration 20, loss = 0.73412478\n",
            "Iteration 21, loss = 0.72008930\n",
            "Iteration 22, loss = 0.70672483\n",
            "Iteration 23, loss = 0.69384329\n",
            "Iteration 24, loss = 0.68130600\n",
            "Iteration 25, loss = 0.66909165\n",
            "Iteration 26, loss = 0.65718525\n",
            "Iteration 27, loss = 0.64564200\n",
            "Iteration 28, loss = 0.63447553\n",
            "Iteration 29, loss = 0.62362461\n",
            "Iteration 30, loss = 0.61308944\n",
            "Iteration 31, loss = 0.60295549\n",
            "Iteration 32, loss = 0.59324184\n",
            "Iteration 33, loss = 0.58394204\n",
            "Iteration 34, loss = 0.57500000\n",
            "Iteration 35, loss = 0.56639103\n",
            "Iteration 36, loss = 0.55810700\n",
            "Iteration 37, loss = 0.55009354\n",
            "Iteration 38, loss = 0.54235460\n",
            "Iteration 39, loss = 0.53488052\n",
            "Iteration 40, loss = 0.52762712\n",
            "Iteration 41, loss = 0.52058722\n",
            "Iteration 42, loss = 0.51374092\n",
            "Iteration 43, loss = 0.50708313\n",
            "Iteration 44, loss = 0.50058587\n",
            "Iteration 45, loss = 0.49423163\n",
            "Iteration 46, loss = 0.48801540\n",
            "Iteration 47, loss = 0.48194877\n",
            "Iteration 48, loss = 0.47602583\n",
            "Iteration 49, loss = 0.47026552\n",
            "Iteration 50, loss = 0.46465608\n",
            "Iteration 51, loss = 0.45922506\n",
            "Iteration 52, loss = 0.45397560\n",
            "Iteration 53, loss = 0.44884757\n",
            "Iteration 54, loss = 0.44382482\n",
            "Iteration 55, loss = 0.43888995\n",
            "Iteration 56, loss = 0.43407175\n",
            "Iteration 57, loss = 0.42934897\n",
            "Iteration 58, loss = 0.42470143\n",
            "Iteration 59, loss = 0.42014684\n",
            "Iteration 60, loss = 0.41564817\n",
            "Iteration 61, loss = 0.41123345\n",
            "Iteration 62, loss = 0.40682102\n",
            "Iteration 63, loss = 0.40242144\n",
            "Iteration 64, loss = 0.39801790\n",
            "Iteration 65, loss = 0.39359178\n",
            "Iteration 66, loss = 0.38913364\n",
            "Iteration 67, loss = 0.38465534\n",
            "Iteration 68, loss = 0.38014574\n",
            "Iteration 69, loss = 0.37569493\n",
            "Iteration 70, loss = 0.37147429\n",
            "Iteration 71, loss = 0.36762879\n",
            "Iteration 72, loss = 0.36398714\n",
            "Iteration 73, loss = 0.36042387\n",
            "Iteration 74, loss = 0.35695192\n",
            "Iteration 75, loss = 0.35351962\n",
            "Iteration 76, loss = 0.35010931\n",
            "Iteration 77, loss = 0.34671692\n",
            "Iteration 78, loss = 0.34334882\n",
            "Iteration 79, loss = 0.33999915\n",
            "Iteration 80, loss = 0.33667112\n",
            "Iteration 81, loss = 0.33336430\n",
            "Iteration 82, loss = 0.33007934\n",
            "Iteration 83, loss = 0.32681766\n",
            "Iteration 84, loss = 0.32358781\n",
            "Iteration 85, loss = 0.32039134\n",
            "Iteration 86, loss = 0.31722925\n",
            "Iteration 87, loss = 0.31409902\n",
            "Iteration 88, loss = 0.31100522\n",
            "Iteration 89, loss = 0.30795803\n",
            "Iteration 90, loss = 0.30494679\n",
            "Iteration 91, loss = 0.30197659\n",
            "Iteration 92, loss = 0.29904720\n",
            "Iteration 93, loss = 0.29616342\n",
            "Iteration 94, loss = 0.29332093\n",
            "Iteration 95, loss = 0.29051095\n",
            "Iteration 96, loss = 0.28772046\n",
            "Iteration 97, loss = 0.28495308\n",
            "Iteration 98, loss = 0.28222454\n",
            "Iteration 99, loss = 0.27952215\n",
            "Iteration 100, loss = 0.27684853\n",
            "Iteration 101, loss = 0.27416573\n",
            "Iteration 102, loss = 0.27146364\n",
            "Iteration 103, loss = 0.26874204\n",
            "Iteration 104, loss = 0.26606042\n",
            "Iteration 105, loss = 0.26348325\n",
            "Iteration 106, loss = 0.26093374\n",
            "Iteration 107, loss = 0.25840143\n",
            "Iteration 108, loss = 0.25588960\n",
            "Iteration 109, loss = 0.25339712\n",
            "Iteration 110, loss = 0.25092218\n",
            "Iteration 111, loss = 0.24847316\n",
            "Iteration 112, loss = 0.24604229\n",
            "Iteration 113, loss = 0.24363069\n",
            "Iteration 114, loss = 0.24123759\n",
            "Iteration 115, loss = 0.23887356\n",
            "Iteration 116, loss = 0.23653400\n",
            "Iteration 117, loss = 0.23421944\n",
            "Iteration 118, loss = 0.23192496\n",
            "Iteration 119, loss = 0.22964967\n",
            "Iteration 120, loss = 0.22739160\n",
            "Iteration 121, loss = 0.22515385\n",
            "Iteration 122, loss = 0.22293829\n",
            "Iteration 123, loss = 0.22074738\n",
            "Iteration 124, loss = 0.21858275\n",
            "Iteration 125, loss = 0.21644110\n",
            "Iteration 126, loss = 0.21432300\n",
            "Iteration 127, loss = 0.21223029\n",
            "Iteration 128, loss = 0.21016376\n",
            "Iteration 129, loss = 0.20812168\n",
            "Iteration 130, loss = 0.20610382\n",
            "Iteration 131, loss = 0.20411264\n",
            "Iteration 132, loss = 0.20214359\n",
            "Iteration 133, loss = 0.20019473\n",
            "Iteration 134, loss = 0.19826883\n",
            "Iteration 135, loss = 0.19636734\n",
            "Iteration 136, loss = 0.19448820\n",
            "Iteration 137, loss = 0.19262575\n",
            "Iteration 138, loss = 0.19077061\n",
            "Iteration 139, loss = 0.18892869\n",
            "Iteration 140, loss = 0.18709898\n",
            "Iteration 141, loss = 0.18528566\n",
            "Iteration 142, loss = 0.18348921\n",
            "Iteration 143, loss = 0.18171041\n",
            "Iteration 144, loss = 0.17996295\n",
            "Iteration 145, loss = 0.17823523\n",
            "Iteration 146, loss = 0.17654324\n",
            "Iteration 147, loss = 0.17492889\n",
            "Iteration 148, loss = 0.17335564\n",
            "Iteration 149, loss = 0.17176328\n",
            "Iteration 150, loss = 0.17016448\n",
            "Iteration 151, loss = 0.16856677\n",
            "Iteration 152, loss = 0.16700327\n",
            "Iteration 153, loss = 0.16548189\n",
            "Iteration 154, loss = 0.16399170\n",
            "Iteration 155, loss = 0.16254519\n",
            "Iteration 156, loss = 0.16110443\n",
            "Iteration 157, loss = 0.15966522\n",
            "Iteration 158, loss = 0.15822781\n",
            "Iteration 159, loss = 0.15679330\n",
            "Iteration 160, loss = 0.15537736\n",
            "Iteration 161, loss = 0.15398368\n",
            "Iteration 162, loss = 0.15260305\n",
            "Iteration 163, loss = 0.15123639\n",
            "Iteration 164, loss = 0.14989109\n",
            "Iteration 165, loss = 0.14856455\n",
            "Iteration 166, loss = 0.14725880\n",
            "Iteration 167, loss = 0.14597394\n",
            "Iteration 168, loss = 0.14470749\n",
            "Iteration 169, loss = 0.14345728\n",
            "Iteration 170, loss = 0.14222310\n",
            "Iteration 171, loss = 0.14100047\n",
            "Iteration 172, loss = 0.13979119\n",
            "Iteration 173, loss = 0.13859332\n",
            "Iteration 174, loss = 0.13740834\n",
            "Iteration 175, loss = 0.13623343\n",
            "Iteration 176, loss = 0.13508448\n",
            "Iteration 177, loss = 0.13395151\n",
            "Iteration 178, loss = 0.13283175\n",
            "Iteration 179, loss = 0.13172581\n",
            "Iteration 180, loss = 0.13063415\n",
            "Iteration 181, loss = 0.12955689\n",
            "Iteration 182, loss = 0.12849241\n",
            "Iteration 183, loss = 0.12744016\n",
            "Iteration 184, loss = 0.12640144\n",
            "Iteration 185, loss = 0.12537659\n",
            "Iteration 186, loss = 0.12436567\n",
            "Iteration 187, loss = 0.12336844\n",
            "Iteration 188, loss = 0.12238471\n",
            "Iteration 189, loss = 0.12141460\n",
            "Iteration 190, loss = 0.12045876\n",
            "Iteration 191, loss = 0.11951512\n",
            "Iteration 192, loss = 0.11858430\n",
            "Iteration 193, loss = 0.11766553\n",
            "Iteration 194, loss = 0.11675991\n",
            "Iteration 195, loss = 0.11586235\n",
            "Iteration 196, loss = 0.11497677\n",
            "Iteration 197, loss = 0.11410241\n",
            "Iteration 198, loss = 0.11323918\n",
            "Iteration 199, loss = 0.11238530\n",
            "Iteration 200, loss = 0.11154112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=200, verbose=True)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(x_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiM7KWvC-QfO",
        "outputId": "313b091b-7445-4d50-84ec-fe202373822a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9821428571428571"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=model.predict(x_test)\n",
        "\n",
        "y_pred                   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3P84hEiAVZb",
        "outputId": "a8502e7e-953f-41bc-96e4-d361ed2df4e4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 1,\n",
              "       0, 0, 2, 0, 0, 1, 1, 0, 2, 2, 0, 2, 2, 1, 0, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(x_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05iHfCpNBKpI",
        "outputId": "2d825b24-04f8-43ac-9250-4f56589b1353"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9473684210526315"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_pred,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9spMOlDnDi2Z",
        "outputId": "d81d4038-304e-4eaf-b7a6-b49c8e716557"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9473684210526315"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}